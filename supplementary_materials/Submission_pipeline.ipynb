{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook contains all steps used to compute the heuristics-based features on training dataset.\n",
        "- Files that is needed on side to run this code: train_neg_full, train_pos_full, emoticon15.csv"
      ],
      "metadata": {
        "id": "sOzpc6FcOKzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPEkhIymOUXd",
        "outputId": "4e12ed68-962d-4337-8370-7ebd7329ea8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install nltk\n",
        "!pip install pandas\n",
        "!pip install scipy\n",
        "!pip install scikit-learn\n",
        "!pip install ekphrasis"
      ],
      "metadata": {
        "id": "1Hn1RRL9OUNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdrJhJHQOIWt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "with open(\"train_neg_full.txt\") as f:\n",
        "    neg_txt = [line for line in f]\n",
        "    neg_df = pd.DataFrame(neg_txt, columns=['line']).drop_duplicates(keep='first')\n",
        "    #print(neg_df.head())\n",
        "f.close()\n",
        "\n",
        "with open(\"train_pos_full.txt\") as f:\n",
        "    pos_txt = [line for line in f]\n",
        "    pos_df = pd.DataFrame(pos_txt, columns=['line']).drop_duplicates(keep='first')\n",
        "    #print(pos_df.head())\n",
        "f.close\n",
        "\n",
        "pos_df['label'] = 1\n",
        "neg_df['label'] = -1\n",
        "\n",
        "full_df_orig= pd.concat([pos_df,neg_df],ignore_index=True)\n",
        "full_df_orig.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_emoji = pd.read_csv('emoticon15.csv')\n",
        "\n",
        "total_emojis =[]\n",
        "\n",
        "emoji_dictionary = pd.DataFrame(columns=['emoji','meaning', 'sentiment'])\n",
        "\n",
        "for i in range(len(df_emoji['icons'])):\n",
        "  str_list_of_emojis = df_emoji['icons'][i]\n",
        "  str_list_of_emojis = str_list_of_emojis[1:-1]\n",
        "  #print(str_list_of_emojis)\n",
        "  list_of_emojis = str_list_of_emojis.split(\",\")\n",
        "  #print(list_of_emojis)\n",
        "  list_of_emojis = [emoji.strip() for emoji in list_of_emojis]\n",
        "  #print(list_of_emojis)\n",
        "  list_of_emojis = [emoji[1:-1] for emoji in list_of_emojis]\n",
        "  #print(list_of_emojis)\n",
        "  for emoji in list_of_emojis:\n",
        "    emoji_dictionary.loc[len(emoji_dictionary)] = [emoji, \" \"+df_emoji['meaning'][i].lower()+\" \", df_emoji['Sentiment'][i]]\n",
        "\n",
        "\n",
        "  total_emojis.extend(list_of_emojis)\n",
        "print(total_emojis)"
      ],
      "metadata": {
        "id": "GCqOPMHIOlOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emojis_with_parentheses = []\n",
        "for emoji in total_emojis:\n",
        "  if '(' in emoji or ')' in emoji:\n",
        "    if emoji != \":)\" and emoji != \":(\" and emoji != \":((\" and emoji != \":))\":\n",
        "      emojis_with_parentheses.append(emoji)\n",
        "print(emojis_with_parentheses)"
      ],
      "metadata": {
        "id": "zWD0CXPaOqZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0 : Emoji score calculation"
      ],
      "metadata": {
        "id": "JjL1hoQ4Oqzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "corpus_orig = []\n",
        "for i in tqdm(range(len(full_df_orig))):\n",
        "  review = full_df_orig['line'][i]\n",
        "  review = review.lower()\n",
        "  corpus_orig.append(review)\n",
        "\n",
        "\n",
        "emoji_scores = []\n",
        "corpus_after_emoji_replacement =[]\n",
        "for index, line in enumerate(corpus_orig):\n",
        "    words = set(line.replace(\"\\n\",\"\").split(\" \"))\n",
        "\n",
        "    emoji_score =0\n",
        "    present_emoji = words & set(total_emojis)\n",
        "    if len(present_emoji) > 0:\n",
        "        for emoji in present_emoji:\n",
        "          row = emoji_dictionary.loc[emoji_dictionary['emoji'] == emoji]\n",
        "          replacement = row['meaning'].values[0]\n",
        "          line = line.replace(emoji, replacement)\n",
        "\n",
        "          emoji_score+= emoji_dictionary[emoji_dictionary['emoji'] == emoji]['sentiment'].values[0]\n",
        "\n",
        "    corpus_after_emoji_replacement.append(line)\n",
        "    emoji_scores.append(emoji_score)\n",
        "\n",
        "\n",
        "print(len(emoji_scores))\n",
        "print(len(corpus_after_emoji_replacement))"
      ],
      "metadata": {
        "id": "ate6sK_6OuVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_scores = []\n",
        "\n",
        "for index, line in enumerate(corpus_orig):\n",
        "    words = set(line.replace(\"\\n\",\"\").split(\" \"))\n",
        "\n",
        "    emoji_score =0\n",
        "    present_emoji = words & set(total_emojis)\n",
        "    if len(present_emoji) > 0:\n",
        "        for emoji in present_emoji:\n",
        "          emoji_score+= emoji_dictionary[emoji_dictionary['emoji'] == emoji]['sentiment'].values[0]\n",
        "\n",
        "    for emoji in emojis_with_parentheses:\n",
        "      replacement = emoji_dictionary[emoji_dictionary['emoji'] == emoji]['sentiment'].values[0]\n",
        "      line = line.replace(emoji, \"\")\n",
        "    emoji_scores.append(emoji_score)\n",
        "\n",
        "\n",
        "print(len(emoji_scores))\n",
        "\n",
        "df_with_emoji_scores = full_df_orig.copy()\n",
        "df_with_emoji_scores.loc[:,'emoji_scores'] = emoji_scores\n",
        "df_with_emoji_scores.loc[:,'line'] = corpus_after_emoji_replacement"
      ],
      "metadata": {
        "id": "033jJMWqPu-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_with_emoji_scores.to_csv('ff_train_data_with_emoji_scores.csv')"
      ],
      "metadata": {
        "id": "rgL5btYhP5WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def twitter_specific_analysis(line,num_replaced_lines):\n",
        "\n",
        "  replaced = False\n",
        "  if \"( <user> live on <url>\" in line:\n",
        "    line = line.replace(\"( <user> live on <url>\", \"\")\n",
        "    replaced = True\n",
        "\n",
        "  if \"( <user> <url>\" in line:\n",
        "    line = line.replace(\"( <user> <url>\", \"\")\n",
        "    replaced = True\n",
        "\n",
        "  if \"( <url>\" in line:\n",
        "    line = line.replace(\"( <url>\",\"\")\n",
        "    replaced = True\n",
        "  if \"( live at <url>\" in line:\n",
        "    line = line.replace(\"( live at <url>\",\"\")\n",
        "    replaced = True\n",
        "\n",
        "  if \"( rt <user>\" in line:\n",
        "    line = line.replace(\"( rt <user>\",\"\")\n",
        "    replaced = True\n",
        "  \"\"\"\n",
        "  if \"<url> via <user>\" in line:\n",
        "    line = line.replace(\"<url> via <user>\",\"\")\n",
        "    replaced = True\n",
        "  \"\"\"\n",
        "  if \"( via <user> <url>\" in line:\n",
        "    line = line.replace(\"( via <user> <url>\",\"\")\n",
        "    replaced = True\n",
        "\n",
        "  if \"( via <user>\" in line:\n",
        "    line = line.replace(\"( via <user>\",\"\")\n",
        "    replaced = True\n",
        "\n",
        "  if \"< ^ , ^ )\" in line:\n",
        "    line = line.replace(\"< ^ , ^ )\",\"happy\")\n",
        "    replaced = True\n",
        "  return line,num_replaced_lines"
      ],
      "metadata": {
        "id": "x4NEIvUtRsOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_dyck_language(s):\n",
        "    # Check if the string contains only a single opening parenthesis \"(\"\n",
        "    if s.endswith(\"... <url>\"):\n",
        "      # Find the position of the last occurrence of '('\n",
        "      pos = s.rfind('(')\n",
        "\n",
        "      # If the character is found, replace it\n",
        "      if pos != -1:\n",
        "          s = s[:pos] + '' + s[pos+1:]\n",
        "      # Replace last opening bracket\n",
        "      # Also if it contains just one opening bracket - don't count it in\n",
        "    elif s.count(\"(\") == 1 and s.count(\")\") == 0:\n",
        "      return s,None\n",
        "\n",
        "\n",
        "    stack = []\n",
        "    for index, char in enumerate(s):\n",
        "        if char == '(':\n",
        "            stack.append((char, index))\n",
        "        elif char == ')':\n",
        "            if not stack:\n",
        "                return s,(index, ')')\n",
        "            stack.pop()\n",
        "\n",
        "    if stack:\n",
        "        _, index = stack.pop()\n",
        "        return s,(index, '(')\n",
        "    else:\n",
        "        return s,None"
      ],
      "metadata": {
        "id": "IK4Jqm53RvHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_labels =[]\n",
        "new_corpus = []\n",
        "emoji_scores = [score for score in df_with_emoji_scores['emoji_scores']]\n",
        "new_emoji_scores = []\n",
        "#STATS\n",
        "num_replaced_lines=0\n",
        "pos_emoji_added = 0\n",
        "neg_emoji_added = 0\n",
        "num_replaced_lines=0\n",
        "\n",
        "for i, line in enumerate(corpus_orig):\n",
        "  emoji_score = emoji_scores[i]\n",
        "\n",
        "  pred_fake_lables = 0\n",
        "\n",
        "  line1,num_replaced_lines = twitter_specific_analysis(line,num_replaced_lines)\n",
        "\n",
        "  # Delete first parenthesis and Insert fake label\n",
        "  line2,res = detect_dyck_language(line1)\n",
        "\n",
        "  if res is not None:\n",
        "    index,char = res\n",
        "\n",
        "    if char == '(':\n",
        "      pred_fake_lables = -1\n",
        "      neg_emoji_added+=1\n",
        "      emoji_score -= 0.4\n",
        "      #print(newline)\n",
        "    else:\n",
        "      pred_fake_lables = 1\n",
        "      pos_emoji_added+=1\n",
        "      emoji_score += 0.4\n",
        "      #print(newline)\n",
        "\n",
        "  fake_labels.append(pred_fake_lables)\n",
        "  new_corpus.append(line2)\n",
        "  new_emoji_scores.append(emoji_score)\n",
        "\n",
        "\n",
        "print(f\"{len(new_corpus)}\")\n",
        "print(f\"{len(fake_labels)}\")\n",
        "print(f\"{len(new_emoji_scores)}\")\n",
        "print(f\"pos_emoji_added: {pos_emoji_added}\")\n",
        "print(f\"neg_emoji_added: {neg_emoji_added}\")\n",
        "print(f\"num_replaced_lines: {num_replaced_lines}\")"
      ],
      "metadata": {
        "id": "LSJ4ZB3CRw43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_step2_with_fakelables = df_with_emoji_scores.copy()\n",
        "df_step2_with_fakelables.loc[:,'line'] = new_corpus\n",
        "df_step2_with_fakelables.loc[:,'emoji_scores'] = new_emoji_scores\n",
        "df_step2_with_fakelables.loc[:,'fake_lables'] = fake_labels"
      ],
      "metadata": {
        "id": "0D_S8gY_R1iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing before advertisement score calculation"
      ],
      "metadata": {
        "id": "FfGTGS6FS0c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import re\n",
        "\n",
        "def process_string(input_string:str):\n",
        "    # Find all hashtags\n",
        "    hashtags = re.findall(r'<hashtag>(.*?)</hashtag>', input_string)\n",
        "\n",
        "    # Split the string by hashtags\n",
        "    parts = re.split(r'<hashtag>.*?</hashtag>', input_string)\n",
        "\n",
        "    # Remove empty strings from parts\n",
        "    parts = [part.strip() for part in parts if part.strip()]\n",
        "\n",
        "    # Format the output list\n",
        "    hashtags_written_out = input_string.replace('<hashtag> ', '').replace('</hashtag> ', '')\n",
        "    no_hashtags = ''.join(parts) if '<hashtag>' in input_string else ''\n",
        "    output = [hashtags_written_out] + [no_hashtags] + [hashtags]\n",
        "\n",
        "    return output\n",
        "\n",
        "def preprocess(tweets:list):\n",
        "\n",
        "    text_processor = TextPreProcessor(\n",
        "        fix_html=True,\n",
        "        segmenter=\"twitter\",\n",
        "        corrector=\"twitter\",\n",
        "        tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "        dicts=[emoticons],\n",
        "        unpack_hashtags=True,\n",
        "        unpack_contractions=True,\n",
        "        spell_correct_elong=True,\n",
        "        normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "        annotate={'hashtag','number'}\n",
        "    )\n",
        "\n",
        "    pre_process_1 = [\" \".join(text_processor.pre_process_doc(tweet)) for tweet in tweets]\n",
        "    pre_process_2 = [process_string(tweet) for tweet in pre_process_1]\n",
        "\n",
        "    return pre_process_2"
      ],
      "metadata": {
        "id": "M1b7CPiNR3j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advertisement score calculation"
      ],
      "metadata": {
        "id": "v2PuwY0fS5pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count keywords in a line\n",
        "def count_keywords(line, keywords):\n",
        "    count = 0\n",
        "    for keyword in keywords:\n",
        "        count += line.lower().count(keyword)\n",
        "    return count\n",
        "\n",
        "def spam_detection(line):\n",
        "  listOfElectronics = [\" apple \",\" samsung \",\" lenovo \",\" hp \",\" microsoft \",\" sony \",\" logitech \", \" asus \",\" amazon \",\" dell \",\" acer \" ,\" intel \",\" simpletech \",\" belkin \",\" gigabyte \"]\n",
        "  book_keywords = [\" paperback \", \" hardcover \",\" academic edition \",\" kindle \",\" audio cd \",\" cd-rom \",\" dvd \",\" thesaurus edition \",\" collector's edition \",\"th edition \",\"nd edition \"]\n",
        "  electronics_keywords = [\" cable \",\" htmi \",\" audio \",\" adapter \",\" conductor \",\" gb \", \" fiber \",\" feet \",\" m \",\" inch \"]\n",
        "\n",
        "  counts = line.count(\"<number>\")\n",
        "\n",
        "  counts += sum(char.isdigit() for char in line)\n",
        "\n",
        "  if any(x in line for x in book_keywords):\n",
        "    if \"<url>\" in line:\n",
        "          return True\n",
        "\n",
        "  elif \" frame \" in line and \"<url>\" in line and counts>0:\n",
        "      return True\n",
        "\n",
        "  elif any(x in line for x in listOfElectronics):\n",
        "      if \"<url>\" in line and counts>0:\n",
        "        #print(line)\n",
        "        return True\n",
        "  elif \"<url>\" in line:\n",
        "      # Initialize counters\n",
        "      lines_with_keywords_count = 0\n",
        "      total_keyword_occurrences = 0\n",
        "\n",
        "      # Iterate through lines and count keywords\n",
        "      keyword_count = count_keywords(line, electronics_keywords)\n",
        "      if keyword_count > 0:\n",
        "          lines_with_keywords_count += 1\n",
        "          total_keyword_occurrences += keyword_count\n",
        "      if total_keyword_occurrences>=2:\n",
        "        return True\n",
        "      return False\n",
        "  else:\n",
        "    return False"
      ],
      "metadata": {
        "id": "GexS-60HR-o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [line for line in df_step2_with_fakelables['line']]\n",
        "res = preprocess(corpus)\n",
        "corpus_with_no_num = []\n",
        "for triple in res:\n",
        "  corpus_with_no_num.append(triple[0])\n",
        "spam_scores = []\n",
        "for i, line in enumerate(corpus_with_no_num):\n",
        "    # do spam detection\n",
        "  score = spam_detection(line)\n",
        "\n",
        "  if score:\n",
        "    spam_pred = 1\n",
        "  else:\n",
        "    spam_pred = 0\n",
        "  spam_scores.append(spam_pred)\n",
        "df_step3_with_spamscore= df_step2_with_fakelables.copy()\n",
        "df_step3_with_spamscore.loc[:,'spam_score'] = spam_scores\n",
        "final_corpus = []\n",
        "corpus_before = [line for line in df_step3_with_spamscore['line']]\n",
        "for i in range(len(corpus_before)):\n",
        "  preprocessed_line = corpus_before[i]\n",
        "  pattern = r'[^a-zA-Z<>.,!? ]'\n",
        "  # Use re.sub() to replace the matched characters with an empty string\n",
        "  preprocessed_line = re.sub(pattern, '', preprocessed_line)\n",
        "  preprocessed_line = preprocessed_line.split()\n",
        "  preprocessed_line = ' '.join(preprocessed_line)\n",
        "  final_corpus.append(preprocessed_line)\n",
        "df_step3_with_spamscore.loc[:,'line'] = final_corpus"
      ],
      "metadata": {
        "id": "5_Po6tmqSHbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We divided our training set in to subsets, so we can calculate the scores in smaller steps, which was better for us to run on colab"
      ],
      "metadata": {
        "id": "OOG7s0tUSRNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "resul_df = df_step3_with_spamscore.copy()\n",
        "resul_df.head()\n",
        "from tqdm import tqdm\n",
        "def divide_into_sets(input_list,num):\n",
        "    # Calculate the size of each set\n",
        "    set_size = len(input_list) // num\n",
        "    remainder = len(input_list) % num\n",
        "\n",
        "    # Initialize the list of sets\n",
        "    sets = []\n",
        "    start = 0\n",
        "\n",
        "    for i in range(num):\n",
        "        end = start + set_size + (1 if i < remainder else 0)\n",
        "        sets.append(input_list[start:end])\n",
        "        print(f'start {start} and end {end} indices for list {i}')\n",
        "        start = end\n",
        "\n",
        "    return sets\n",
        "corpus= []\n",
        "count =0\n",
        "for i in tqdm(range(len(resul_df))):\n",
        "\n",
        "  review = resul_df['line'][i]\n",
        "  if isinstance(resul_df['line'][i],float):\n",
        "    review =\"\"\n",
        "    count+=1\n",
        "  corpus.append(review)\n",
        "divided_list = divide_into_sets(corpus,12)\n",
        "for j in range(12):\n",
        "  filename = f\"divided_lists_{j}.txt\"\n",
        "  with open(filename, 'w') as file:\n",
        "        file.write(str(divided_list[j]))"
      ],
      "metadata": {
        "id": "dgJHVfsNSgYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 : Emotion score calculation"
      ],
      "metadata": {
        "id": "ONmvVcnrTDJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Y3Ip2OqpTFIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_emotion(emotion_scores):\n",
        "    positive_emotions = {'joy', 'love', 'optimism', 'trust', 'anticipation', 'surprise'}\n",
        "    negative_emotions = {'anger', 'disgust', 'fear', 'pessimism', 'sadness'}\n",
        "\n",
        "    # Flatten the list if necessary (assuming input structure)\n",
        "    if isinstance(emotion_scores, list) and len(emotion_scores) == 1:\n",
        "        emotion_scores = emotion_scores[0]\n",
        "\n",
        "    # Find the label with the highest score\n",
        "    max_score = max(emotion_scores, key=lambda x: x['score'])\n",
        "    max_label = max_score['label']\n",
        "\n",
        "    # Classify as positive or negative\n",
        "    if max_label in positive_emotions:\n",
        "        classification = 1\n",
        "    elif max_label in negative_emotions:\n",
        "        classification = -1\n",
        "\n",
        "    return max_label, classification\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-emotion-multilabel-latest\", return_all_scores=True,device =0)\n",
        "import json\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "def iterate(j):\n",
        "    filename = f\"divided_lists_{j}.txt\"\n",
        "    with open(filename, 'r') as file:\n",
        "        corpus_subset = eval(file.read())\n",
        "\n",
        "    dataset = Dataset.from_dict({\"text\": corpus_subset})\n",
        "    emotion_scores = []\n",
        "\n",
        "    # Define a function to apply the classifier\n",
        "    def classify_text(batch):\n",
        "      with torch.no_grad():\n",
        "        r = pipe(batch[\"text\"])\n",
        "        return {\"results\": r}\n",
        "\n",
        "    # Apply the classifier to the dataset\n",
        "    results = dataset.map(classify_text, batched=True,batch_size = 64)\n",
        "    results_list = [row['results'] for row in results]\n",
        "\n",
        "    emotion_scores = []\n",
        "    for res in results_list:\n",
        "        pred_labels = classify_emotion(res)\n",
        "        emotion_scores.append(pred_labels[1])\n",
        "\n",
        "    print(len(emotion_scores))\n",
        "    print(emotion_scores[:100])\n",
        "\n",
        "    filename = f\"emotion_scores_{j}.txt\"\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(str(emotion_scores))\n"
      ],
      "metadata": {
        "id": "1T0xbyWhTG7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(12):\n",
        "  iterate(i)"
      ],
      "metadata": {
        "id": "iSsA0C7sT9fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment classification calculation with XLM-T"
      ],
      "metadata": {
        "id": "a6oCx9y_UCMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import torch\n",
        "import time\n",
        "# Preprocess text (username and link placeholders)\n",
        "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "# PT\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "B1y0F6cJUBbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "def calc_sentiment_batch(batch):\n",
        "    encoded_input = tokenizer(batch['text'], padding=True, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded_input)\n",
        "    scores = output.logits.detach().cpu().numpy()\n",
        "    sentiments = []\n",
        "\n",
        "    for score in scores:\n",
        "        score = softmax(score)\n",
        "        ranking = np.argsort(score)[::-1]\n",
        "        firstclass = ranking[0]\n",
        "        if firstclass == 1:\n",
        "            firstclass = ranking[1]\n",
        "        pred = 1 if firstclass == 2 else -1\n",
        "        sentiments.append(pred)\n",
        "\n",
        "    batch['sentiment'] = sentiments\n",
        "    return batch"
      ],
      "metadata": {
        "id": "9mJIckx2UHr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def calculate(j):\n",
        "  from datasets import Dataset\n",
        "  import time\n",
        "  # Load the dataset\n",
        "  filename = f\"divided_lists_{j}.txt\"\n",
        "  with open(filename, 'r') as file:\n",
        "      corpus_subset = json.load(file)\n",
        "\n",
        "  # Convert the list to a Hugging Face dataset\n",
        "  dataset = Dataset.from_dict({'text': corpus_subset})\n",
        "\n",
        "  # Apply the sentiment analysis function\n",
        "  start_time = time.time()\n",
        "  dataset = dataset.map(calc_sentiment_batch, batched=True, batch_size=32)\n",
        "  end_time = time.time()\n",
        "  print(f\"Processing time: {end_time - start_time} seconds\")\n",
        "\n",
        "  # Save the results\n",
        "  filename = f\"sentiment_scores_{j}.txt\"\n",
        "  with open(filename, 'w') as file:\n",
        "      file.write(str(dataset['sentiment']))"
      ],
      "metadata": {
        "id": "_8cj7XAkUJY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vadar predictions for comparison"
      ],
      "metadata": {
        "id": "uOwPcPr4UQ9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from tqdm import tqdm\n",
        "df_step4_with_vadar = df_step3_with_spamscore.copy()\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "corpus = []\n",
        "for i in tqdm(range(len(df_step4_with_vadar))):\n",
        "\n",
        "  review = df_step3_with_spamscore['line'][i]\n",
        "  if isinstance(df_step4_with_vadar['line'][i],float):\n",
        "    review =\"\"\n",
        "  corpus.append(review)\n",
        "\n",
        "vadars = []\n",
        "for text in corpus:\n",
        "  scores = analyzer.polarity_scores(text)\n",
        "  score = scores['compound']\n",
        "  vadars.append(score)\n",
        "\n",
        "print(len(vadars))\n",
        "df_step4_with_vadar['vadar'] = vadars"
      ],
      "metadata": {
        "id": "leSX0rBbUTkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving all predictions in one file"
      ],
      "metadata": {
        "id": "m_geUbahU5HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_scores =[]\n",
        "sentiment_scores =[]\n",
        "for i in range(12):\n",
        "    file_path = f\"emotion_scores_{i}.txt\"\n",
        "    with open(file_path,\"r\") as file:\n",
        "        emotions = eval(file.read())\n",
        "    #print(len(emotions))\n",
        "    emotion_scores.extend(emotions)\n",
        "\n",
        "print(len(emotion_scores))\n",
        "for i in range(12):\n",
        "    file_path = f\"sentiment_scores_{i}.txt\"\n",
        "    with open(file_path,\"r\") as file:\n",
        "        sentiments = eval(file.read())\n",
        "    #print(len(emotions))\n",
        "    sentiment_scores.extend(sentiments)\n",
        "\n",
        "print(len(sentiment_scores))\n",
        "\n",
        "submission_df = df_step4_with_vadar.copy()\n",
        "\n",
        "submission_df.loc[:,'emotion_scores'] = emotion_scores\n",
        "submission_df.loc[:,'sentiment_scores'] = sentiment_scores\n",
        "submission_df.to_csv('submission_train.csv',index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXRV6U2NVOIc",
        "outputId": "ab9c84ec-cd8f-480e-e82a-1ac3b98e7e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2270482\n",
            "2270482\n"
          ]
        }
      ]
    }
  ]
}