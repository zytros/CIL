{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## In This code we show accuracy for different ensemble methods"
      ],
      "metadata": {
        "id": "FFpWs4n7eigZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXhVIatwdy-M",
        "outputId": "f7fecbdc-4bfe-4519-d245-6be9544b2294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_scores =[]\n",
        "sentiment_scores =[]\n",
        "for i in range(12):\n",
        "    file_path = f\"emotion_scores_{i}.txt\"\n",
        "    with open(file_path,\"r\") as file:\n",
        "        emotions = eval(file.read())\n",
        "    #print(len(emotions))\n",
        "    emotion_scores.extend(emotions)\n",
        "\n",
        "print(len(emotion_scores))\n",
        "for i in range(12):\n",
        "    file_path = f\"sentiment_scores_{i}.txt\"\n",
        "    with open(file_path,\"r\") as file:\n",
        "        sentiments = eval(file.read())\n",
        "    #print(len(emotions))\n",
        "    sentiment_scores.extend(sentiments)\n",
        "\n",
        "print(len(sentiment_scores))\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "train_df = pd.read_csv('submission_train.csv')\n",
        "samuels_pred_df = pd.read_csv('training_no_duplicates.csv')\n",
        "num_rows =1127644\n",
        "# Split the DataFrame into two parts\n",
        "first_part = train_df.iloc[:num_rows]\n",
        "second_part = train_df.iloc[num_rows:]\n",
        "\n",
        "print(len(first_part))\n",
        "print(len(second_part))\n",
        "print(len(first_part)+len(second_part))\n",
        "# Concatenate the second part first, followed by the first part\n",
        "train_df = pd.concat([second_part, first_part]).reset_index(drop=True)\n",
        "fe = emotion_scores[:1127644]\n",
        "se = emotion_scores[1127644:]\n",
        "emotion_score_swapped = se + fe\n",
        "print(len(emotion_score_swapped))\n",
        "\n",
        "fs = sentiment_scores[:1127644]\n",
        "ss = sentiment_scores[1127644:]\n",
        "sentiment_score_swapped = ss + fs\n",
        "train_df.loc[:, 'emotion_scores'] = emotion_score_swapped\n",
        "\n",
        "train_df.loc[:, 'sentiment_scores2'] = sentiment_score_swapped\n",
        "\n",
        "train_df.loc[:, 'sentiment_scores'] = samuels_pred_df['predictions']\n",
        "train_df.loc[:, 'sentiment_scores'] = train_df.loc[:, 'sentiment_scores'].replace(0,-1)\n",
        "train_df.loc[:, 'emoji_scores'] = train_df['emoji_scores'].replace(0, np.nan)\n",
        "train_df.loc[:,'spam_score'] = train_df['spam_score'].replace(0,np.nan)\n",
        "train_df.loc[:,'fake_lables'] = train_df['fake_lables'].replace(0,np.nan)\n",
        "train_df.loc[:,'spam_score']= train_df['spam_score'].replace(1,-1)\n",
        "def modify_value(x):\n",
        "    if x == 0.0 or np.isnan(x):\n",
        "        return np.nan\n",
        "    elif x > 0.0:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "train_df.loc[:, 'emoji_scores'] = train_df['emoji_scores'].apply(modify_value)\n",
        "train_df.loc[:, 'vadar'] = train_df['vadar'].apply(modify_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA9zyPTAeo3Z",
        "outputId": "4fb9c5cf-0863-4d49-f11f-47fa5973d476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2270482\n",
            "2270482\n",
            "1127644\n",
            "1142838\n",
            "2270482\n",
            "2270482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "old_vadar_score_df = pd.read_csv('full_df_big_deprecated.csv')\n",
        "old_vadar_score_df.loc[:, 'vadar'] = old_vadar_score_df['vadar'].apply(modify_value)\n",
        "vader_scores = old_vadar_score_df['vadar']\n",
        "first_part = vader_scores.iloc[:num_rows]\n",
        "second_part = vader_scores.iloc[num_rows:]\n",
        "\n",
        "vader_scores_swapped = pd.concat([second_part, first_part]).reset_index(drop=True)\n",
        "train_df.loc[:, 'vadar'] = vader_scores_swapped"
      ],
      "metadata": {
        "id": "RSe1-Tz4fGlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measure accuracy with sentiment baseline"
      ],
      "metadata": {
        "id": "OPRVHDgYfLTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Define the target variable y and the features X\n",
        "y = train_df['label']\n",
        "X = train_df[['emoji_scores', 'spam_score', 'fake_lables', 'emotion_scores','sentiment_scores2']]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "params = {\n",
        "    'learning_rate': 0.01,\n",
        "    'max_iter': 300,\n",
        "    'max_leaf_nodes': 127,\n",
        "    'max_depth': 5,  # Choose one of the values from [3, 5, 7]\n",
        "    'min_samples_leaf': 20,\n",
        "    'l2_regularization': 0,\n",
        "    'early_stopping': True\n",
        "}\n",
        "\n",
        "# Initialize the model with the specified parameters\n",
        "model = HistGradientBoostingClassifier(**params)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability scores for the positive class"
      ],
      "metadata": {
        "id": "pA5flzeVfPAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_sam = accuracy_score(y_test, y_pred)\n",
        "precision_sam = precision_score(y_test, y_pred)\n",
        "recall_sam = recall_score(y_test, y_pred)\n",
        "precision_sam_neg = precision_score(y_test, y_pred,pos_label =-1)\n",
        "recall_sam_neg = recall_score(y_test, y_pred,pos_label =-1)\n",
        "print(f\"accuracy: {accuracy_sam:.4f}\")\n",
        "print(f\"precision_pos: {precision_sam:.4f}\")\n",
        "print(f\"recall_pos: {recall_sam:.4f}\")\n",
        "print(f\"precision_neg: {precision_sam_neg:.4f}\")\n",
        "print(f\"recall_neg: {recall_sam_neg:.4f}\")"
      ],
      "metadata": {
        "id": "emOB08gLfZBb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}