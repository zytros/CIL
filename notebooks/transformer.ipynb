{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from MyTransformer import Classifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocessing import preprocess\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 128\n",
    "emb_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(vocab_size=23174, d_model=512, num_heads=8, num_layers=6, d_ff=2048, max_seq_length=128)\n",
    "rand_input = torch.randint(0, 21000, (16,max_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filter_vocab(data, min_count=5):\n",
    "    vocab = {}\n",
    "    for sentence in data:\n",
    "        for word in sentence.split():\n",
    "            vocab[word] = vocab.get(word, 0) + 1\n",
    "    filtered_vocab = {word: count for word, count in vocab.items() if count >= min_count}\n",
    "    return filtered_vocab\n",
    "\n",
    "def build_tokenized_vocab(vocab:dict):\n",
    "    voc = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "    voc['<UNK>'] = len(voc)\n",
    "    voc['<PAD>'] = len(voc)\n",
    "    return voc\n",
    "\n",
    "def pad_lists_in_df_column(df, column_name, desired_length, padding_value):\n",
    "    \"\"\"\n",
    "    Pads lists in a specified column of a DataFrame to a desired length.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to process.\n",
    "    - column_name: The name of the column containing lists to pad.\n",
    "    - desired_length: The desired length of the lists.\n",
    "    - padding_value: The value to use for padding shorter lists.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame with padded lists in the specified column.\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Pad each list in the specified column\n",
    "    df_copy[column_name] = df_copy[column_name].apply(\n",
    "        lambda x: x + [padding_value] * (desired_length - len(x)) if len(x) < desired_length else x\n",
    "    )\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming the DataFrame has two columns: 'features' and 'labels'\n",
    "        # Adjust this method based on the actual structure of your DataFrame\n",
    "        features = self.dataframe.iloc[idx, :-1].values # All columns except the last one\n",
    "        label = self.dataframe.iloc[idx, -1] # Last column\n",
    "        return torch.tensor(features, dtype=torch.int), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# Step 2: Function to create a DataLoader from a DataFrame\n",
    "def create_dataloader_from_df(dataframe, batch_size=32, shuffle=True):\n",
    "    dataset = DataFrameDataset(dataframe)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "def train_model(model, dataloader, n_epochs, loss_fn=nn.BCEWithLogitsLoss(), optimizer=torch.optim.Adam(model.parameters(), lr=0.0001)):\n",
    "    # Check if CUDA is available and move the model to GPU if it is\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            # Move data to the same device as the model\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear the gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = loss_fn(outputs.squeeze(), labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update model parameters\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_csv('preprocessed_pos.csv')\n",
    "df_neg = pd.read_csv('preprocessed_neg.csv')\n",
    "df_pos['label'] = 1\n",
    "df_neg['label'] = 0\n",
    "neg_tweets=df_neg['hashtags_written_out'].values\n",
    "pos_tweets=df_pos['hashtags_written_out'].values\n",
    "all_tweets = np.concatenate((neg_tweets, pos_tweets), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 23174\n"
     ]
    }
   ],
   "source": [
    "vocab = build_filter_vocab(all_tweets, 4)\n",
    "tokenized_vocab = build_tokenized_vocab(vocab)\n",
    "print('vocab size:', len(tokenized_vocab))\n",
    "token_tweets_neg = []\n",
    "for tweet in neg_tweets:\n",
    "    work_tweet = []\n",
    "    for word in tweet.split():\n",
    "        if word in tokenized_vocab:\n",
    "            work_tweet.append(tokenized_vocab[word])\n",
    "        else:\n",
    "            work_tweet.append(tokenized_vocab['<UNK>'])\n",
    "    token_tweets_neg.append(work_tweet)\n",
    "token_tweets_pos = []\n",
    "for tweet in pos_tweets:\n",
    "    work_tweet = []\n",
    "    for word in tweet.split():\n",
    "        if word in tokenized_vocab:\n",
    "            work_tweet.append(tokenized_vocab[word])\n",
    "        else:\n",
    "            work_tweet.append(tokenized_vocab['<UNK>'])\n",
    "    token_tweets_pos.append(work_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0        1        2        3       4       5       6     7  \\\n",
      "0      23172  23172.0      0.0      1.0     2.0     0.0     3.0   0.0   \n",
      "1         17     18.0     19.0     20.0    21.0    22.0    23.0  23.0   \n",
      "2          0     27.0      0.0     28.0    29.0    30.0     9.0  31.0   \n",
      "3         41     18.0     42.0     43.0    44.0    45.0    46.0  18.0   \n",
      "4         41     41.0     41.0     18.0    50.0    45.0    51.0  52.0   \n",
      "...      ...      ...      ...      ...     ...     ...     ...   ...   \n",
      "99995     41    851.0  20693.0    143.0   128.0   129.0   202.0  85.0   \n",
      "99996     41  11251.0      0.0     15.0     0.0     7.0     0.0  96.0   \n",
      "99997     41     41.0   7964.0  23172.0    15.0    15.0    15.0  18.0   \n",
      "99998     41     18.0     47.0     48.0  2207.0   215.0    22.0  23.0   \n",
      "99999     18    338.0  11439.0    143.0     9.0  1870.0  5694.0  68.0   \n",
      "\n",
      "             8        9  ...       58       59       60       61       62  \\\n",
      "0          4.0      5.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "1         24.0     25.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "2         32.0     33.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "3         47.0     48.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "4         53.0     54.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "...        ...      ...  ...      ...      ...      ...      ...      ...   \n",
      "99995  23173.0  23173.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "99996   4623.0      0.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "99997    289.0   2252.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "99998    956.0     59.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "99999    308.0    171.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "\n",
      "            63       64       65       66  label  \n",
      "0      23173.0  23173.0  23173.0  23173.0      0  \n",
      "1      23173.0  23173.0  23173.0  23173.0      0  \n",
      "2      23173.0  23173.0  23173.0  23173.0      0  \n",
      "3      23173.0  23173.0  23173.0  23173.0      0  \n",
      "4      23173.0  23173.0  23173.0  23173.0      0  \n",
      "...        ...      ...      ...      ...    ...  \n",
      "99995  23173.0  23173.0  23173.0  23173.0      1  \n",
      "99996  23173.0  23173.0  23173.0  23173.0      1  \n",
      "99997  23173.0  23173.0  23173.0  23173.0      1  \n",
      "99998  23173.0  23173.0  23173.0  23173.0      1  \n",
      "99999  23173.0  23173.0  23173.0  23173.0      1  \n",
      "\n",
      "[200000 rows x 68 columns]\n"
     ]
    }
   ],
   "source": [
    "df_token_neg = pd.DataFrame(token_tweets_neg)\n",
    "df_token_pos = pd.DataFrame(token_tweets_pos)\n",
    "df_token_neg['label'] = 0\n",
    "df_token_pos['label'] = 1\n",
    "df_token = pd.concat([df_token_neg, df_token_pos])\n",
    "df_token = df_token.fillna(tokenized_vocab['<PAD>'])\n",
    "cols = [col for col in df_token.columns if col != 'label'] + ['label']\n",
    "df_token = df_token[cols]\n",
    "print(df_token)\n",
    "dataloader = create_dataloader_from_df(df_token, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_model(model, dataloader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
