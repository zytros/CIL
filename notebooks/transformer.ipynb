{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from MyTransformer import Classifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocessing import preprocess\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 128 #67\n",
    "emb_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.9491e-11, 1.0000e+00],\n",
       "        [1.6391e-10, 1.0000e+00],\n",
       "        [8.5100e-08, 1.0000e+00],\n",
       "        [9.6047e-08, 1.0000e+00],\n",
       "        [2.1090e-12, 1.0000e+00],\n",
       "        [6.3649e-05, 9.9994e-01],\n",
       "        [9.2682e-10, 1.0000e+00],\n",
       "        [1.3443e-08, 1.0000e+00],\n",
       "        [2.9596e-13, 1.0000e+00],\n",
       "        [6.8242e-12, 1.0000e+00],\n",
       "        [5.5932e-07, 1.0000e+00],\n",
       "        [2.1965e-07, 1.0000e+00],\n",
       "        [4.9675e-06, 9.9999e-01],\n",
       "        [5.4604e-05, 9.9995e-01],\n",
       "        [5.6618e-13, 1.0000e+00],\n",
       "        [4.1563e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier(vocab_size=23174, d_model=512, num_heads=8, num_layers=6, d_ff=2048, max_seq_length=67)\n",
    "rand_input = torch.randint(0, 21000, (16,67))\n",
    "out = model(rand_input)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filter_vocab(data, min_count=5):\n",
    "    vocab = {}\n",
    "    for sentence in data:\n",
    "        for word in sentence.split():\n",
    "            vocab[word] = vocab.get(word, 0) + 1\n",
    "    filtered_vocab = {word: count for word, count in vocab.items() if count >= min_count}\n",
    "    return filtered_vocab\n",
    "\n",
    "def build_tokenized_vocab(vocab:dict):\n",
    "    voc = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "    voc['<UNK>'] = len(voc)\n",
    "    voc['<PAD>'] = len(voc)\n",
    "    return voc\n",
    "\n",
    "def pad_lists_in_df_column(df, column_name, desired_length, padding_value):\n",
    "    \"\"\"\n",
    "    Pads lists in a specified column of a DataFrame to a desired length.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to process.\n",
    "    - column_name: The name of the column containing lists to pad.\n",
    "    - desired_length: The desired length of the lists.\n",
    "    - padding_value: The value to use for padding shorter lists.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame with padded lists in the specified column.\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Pad each list in the specified column\n",
    "    df_copy[column_name] = df_copy[column_name].apply(\n",
    "        lambda x: x + [padding_value] * (desired_length - len(x)) if len(x) < desired_length else x\n",
    "    )\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming the DataFrame has two columns: 'features' and 'labels'\n",
    "        # Adjust this method based on the actual structure of your DataFrame\n",
    "        features = self.dataframe.iloc[idx, :-1].values # All columns except the last one\n",
    "        label = self.dataframe.iloc[idx, -1] # Last column\n",
    "        if label == 0:\n",
    "            label = [1, 0]\n",
    "        else:\n",
    "            label = [0, 1]\n",
    "        return torch.tensor(features, dtype=torch.int), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# Step 2: Function to create a DataLoader from a DataFrame\n",
    "def create_dataloaders_from_df(dataframe, test_size=0.2, batch_size=32, shuffle=True):\n",
    "    # Split the DataFrame into training and testing sets\n",
    "    train_df, test_df = train_test_split(dataframe, test_size=test_size)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DataFrameDataset(train_df)\n",
    "    test_dataset = DataFrameDataset(test_df)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "def train_model(model, train_loader, test_loader, n_epochs, loss_fn=nn.BCEWithLogitsLoss(), optimizer=torch.optim.Adam(model.parameters(), lr=0.0001)):\n",
    "    # Check if CUDA is available and move the model to GPU if it is\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels = batch\n",
    "            # Move data to the same device as the model\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear the gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            #\n",
    "            # print('output dim:', outputs.shape, 'label dim:', labels.shape)\n",
    "            loss = loss_fn(outputs.squeeze(), labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update model parameters\n",
    "            total_loss += loss.item()\n",
    "        print(evaluate_model(model, test_loader))\n",
    "        \n",
    "        \n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in test_loader:\n",
    "            \n",
    "            inputs.to(device)\n",
    "            labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_csv('preprocessed_pos.csv')\n",
    "df_neg = pd.read_csv('preprocessed_neg.csv')\n",
    "df_pos['label'] = 1\n",
    "df_neg['label'] = 0\n",
    "neg_tweets=df_neg['hashtags_written_out'].values\n",
    "pos_tweets=df_pos['hashtags_written_out'].values\n",
    "all_tweets = np.concatenate((neg_tweets, pos_tweets), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 23174\n"
     ]
    }
   ],
   "source": [
    "vocab = build_filter_vocab(all_tweets, 4)\n",
    "tokenized_vocab = build_tokenized_vocab(vocab)\n",
    "print('vocab size:', len(tokenized_vocab))\n",
    "token_tweets_neg = []\n",
    "for tweet in neg_tweets:\n",
    "    work_tweet = []\n",
    "    for word in tweet.split():\n",
    "        if word in tokenized_vocab:\n",
    "            work_tweet.append(tokenized_vocab[word])\n",
    "        else:\n",
    "            work_tweet.append(tokenized_vocab['<UNK>'])\n",
    "    token_tweets_neg.append(work_tweet)\n",
    "token_tweets_pos = []\n",
    "for tweet in pos_tweets:\n",
    "    work_tweet = []\n",
    "    for word in tweet.split():\n",
    "        if word in tokenized_vocab:\n",
    "            work_tweet.append(tokenized_vocab[word])\n",
    "        else:\n",
    "            work_tweet.append(tokenized_vocab['<UNK>'])\n",
    "    token_tweets_pos.append(work_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0        1        2        3       4       5       6     7  \\\n",
      "0      23172  23172.0      0.0      1.0     2.0     0.0     3.0   0.0   \n",
      "1         17     18.0     19.0     20.0    21.0    22.0    23.0  23.0   \n",
      "2          0     27.0      0.0     28.0    29.0    30.0     9.0  31.0   \n",
      "3         41     18.0     42.0     43.0    44.0    45.0    46.0  18.0   \n",
      "4         41     41.0     41.0     18.0    50.0    45.0    51.0  52.0   \n",
      "...      ...      ...      ...      ...     ...     ...     ...   ...   \n",
      "99995     41    851.0  20693.0    143.0   128.0   129.0   202.0  85.0   \n",
      "99996     41  11251.0      0.0     15.0     0.0     7.0     0.0  96.0   \n",
      "99997     41     41.0   7964.0  23172.0    15.0    15.0    15.0  18.0   \n",
      "99998     41     18.0     47.0     48.0  2207.0   215.0    22.0  23.0   \n",
      "99999     18    338.0  11439.0    143.0     9.0  1870.0  5694.0  68.0   \n",
      "\n",
      "             8        9  ...       58       59       60       61       62  \\\n",
      "0          4.0      5.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "1         24.0     25.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "2         32.0     33.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "3         47.0     48.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "4         53.0     54.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "...        ...      ...  ...      ...      ...      ...      ...      ...   \n",
      "99995  23173.0  23173.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "99996   4623.0      0.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "99997    289.0   2252.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "99998    956.0     59.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "99999    308.0    171.0  ...  23173.0  23173.0  23173.0  23173.0  23173.0   \n",
      "\n",
      "            63       64       65       66  label  \n",
      "0      23173.0  23173.0  23173.0  23173.0      0  \n",
      "1      23173.0  23173.0  23173.0  23173.0      0  \n",
      "2      23173.0  23173.0  23173.0  23173.0      0  \n",
      "3      23173.0  23173.0  23173.0  23173.0      0  \n",
      "4      23173.0  23173.0  23173.0  23173.0      0  \n",
      "...        ...      ...      ...      ...    ...  \n",
      "99995  23173.0  23173.0  23173.0  23173.0      1  \n",
      "99996  23173.0  23173.0  23173.0  23173.0      1  \n",
      "99997  23173.0  23173.0  23173.0  23173.0      1  \n",
      "99998  23173.0  23173.0  23173.0  23173.0      1  \n",
      "99999  23173.0  23173.0  23173.0  23173.0      1  \n",
      "\n",
      "[200000 rows x 68 columns]\n"
     ]
    }
   ],
   "source": [
    "df_token_neg = pd.DataFrame(token_tweets_neg)\n",
    "df_token_pos = pd.DataFrame(token_tweets_pos)\n",
    "df_token_neg['label'] = 0\n",
    "df_token_pos['label'] = 1\n",
    "df_token = pd.concat([df_token_neg, df_token_pos])\n",
    "df_token = df_token.fillna(tokenized_vocab['<PAD>'])\n",
    "cols = [col for col in df_token.columns if col != 'label'] + ['label']\n",
    "df_token = df_token[cols]\n",
    "print(df_token)\n",
    "train_loader, test_loader = create_dataloaders_from_df(df_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [26:50<00:00,  3.10it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model(model, train_loader, test_loader, \u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 91\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, n_epochs, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     89\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update model parameters\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluate_model(model, test_loader))\n",
      "Cell \u001b[1;32mIn[25], line 101\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, test_loader)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient calculation\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m--> 101\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m    102\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    103\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\Documents\\CIL\\notebooks\\MyTransformer.py:128\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 128\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEmbeddingTransformer(x)\n\u001b[0;32m    129\u001b[0m     summed_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    130\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(summed_embeddings)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\Documents\\CIL\\notebooks\\MyTransformer.py:108\u001b[0m, in \u001b[0;36mEmbeddingTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_embedding(x)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(x)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, test_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
